# .github/workflows/scrape_jobs.yml
name: Atualização Diária de Vagas

on:
  # Permite rodar manualmente pela interface do GitHub Actions
  workflow_dispatch:

  # Agenda a execução para rodar todo dia às 9h da manhã (horário UTC)
  schedule:
    - cron: '0 9 * * *'

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest

    steps:
      # Passo 1: Clona o seu repositório para a máquina virtual
      - name: Checar o código do repositório
        uses: actions/checkout@v4

      # Passo 2: Configurar o ambiente Python
      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.5'

      # Passo 3: Instalar as dependências do projeto
      - name: Instalar dependências
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Passo 4: Executar o script de scraping
      - name: Executar o Scraper
        run: python scraper.py

      # Passo 5: Fazer o commit do novo arquivo de dados (se houver mudanças)
      - name: Fazer o commit e push dos dados atualizados
        run: |
          # Configura o Git com um usuário bot
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions-bot@github.com"
          
          # --- CORREÇÃO APLICADA AQUI ---
          # 1. Puxa as atualizações mais recentes do repositório
          git pull
          
          # 2. Adiciona os arquivos que podem ter sido modificados
          git add vagas_brasil.csv scraper.log
          
          # 3. Verifica se há mudanças. Se houver, faz o commit e push.
          if ! git diff --staged --quiet; then
            git commit -m "CHORE: Atualiza dados de vagas (`date`)"
            git push
          else
            echo "Nenhuma mudança nos dados para commitar."
          fi
