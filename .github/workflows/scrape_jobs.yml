# .github/workflows/scrape_jobs.yml
name: Atualização Diária de Vagas

on:
  workflow_dispatch:
  schedule:
    - cron: '0 9 * * *'

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checar o código do repositório
        uses: actions/checkout@v4

      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.5'

      # NOVO: Instala o Google Chrome na máquina virtual
      - name: Instalar Google Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Instalar dependências do Python
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Altera o script a ser executado
      - name: Executar todos os Scrapers
        run: python run_scrapers.py

      - name: Fazer o commit e push dos dados atualizados
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions-bot@github.com"
          git pull
          
          # Adiciona o novo arquivo consolidado
          git add vagas_consolidadas.csv *.log
          
          if ! git diff --staged --quiet; then
            git commit -m "CHORE: Atualiza dados de vagas de todas as fontes (`date`)"
            git push
          else
            echo "Nenhuma mudança nos dados para commitar."
          fi
